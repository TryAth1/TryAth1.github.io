---
layout: post
title: Spider Series:1.爬虫介绍
date: 2016-8-25
categories: blog
tags: [爬虫]
description: 本篇是对爬虫常用方法的一些介绍。
---

# Spider系列之爬虫介绍

本篇是对之前一段时间的爬虫学习的一个总结,虽然我也没有爬过很多的网站,但是一些基本的概念还是理解了,希望我接下来的介绍是比较系统的。

## 1.什么是爬虫

互联网就是一张很大的网,爬虫就像一只蜘蛛,趴在网的某个地方,然后通过网之间的连接,一步一步的移动,最终将所有的这张网爬取下来。在我们生活中最常见的就是谷歌,百度这类的搜索引擎,本质上它就是一个非常巨大的爬虫。那么我们为什么要学习爬虫呢?在数据时代,微博这类的社交网站产生了极大的极其丰富的数据值得去探究,比方说可以通过用户的发言,了解股民情绪,从而判断股市的接下来的走势;通过用户某一段时间内的发言,找到这个时间段的潮流,从而为公司的战略设定给出一定的指引。总之,随着数据沉淀的越来越多,对这部分数据如何进行有效的爬取与分析,能够产生巨大的价值。

## 2.爬虫所需的基础知识

- Python,熟悉面向对象编程,数据结构(list,dic,tuple...),for 循环,while循环...
- 几个常用的库,urllib,urllib2,BeautifulSoup,requests,re等...
- 网页结构...

## 3.最简单的爬虫

### 1.urllib2和requests初识

```python
import urllib2 #urllib2库是用来获取并解析网页的
url="https://www.baidu.com" #网站,即我们需要爬取的网站
response=urllib2.urlopen(url) #urllib2.urlopen()方法打开url
content=response.read()#read()方法解析返回的数据
```

上面这个例子中,简单讲解一下就是,我们通过python程序模拟浏览器的访问,平常我们浏览器是怎么访问网站的?首先浏览器向网站的服务器发送请求,然后服务器向浏览器返回数据,然后浏览器解析过后就变成我们所看的网站模样。在这边也是一样的,我们通过urllib2.urlopen(url),向url的服务器发送请求,url的服务器返回数据,然后我们用response.read()解析,我们可以print content看看,会发现和在网页上鼠标右键,查看网页元素所看到的一大串带代码是一样的。

```python
#不过我还是比较推荐用requests库,能够实现的功能实际上差不多,但我觉得requests比较友好一点,那么用requests写如下:
import requests
url="https://www.baidu.com"
content=resquests.get(url).content
#为什么我说requests比较友好一点,在接下来会进行说明
```

### 2.Beautiful Soup is coming

Beautiful Soup也是非常友好的一个库,前面讲到了,content=resquests.get(url).content,返回了网页解析后的数据,但是一看就非常乱,通常我们要从这里面找出数据的话,需要用到正则表达式,但正则表达式也挺麻烦的,因为用正则提取数据后,还要进行进一步的处理,所以我们这边就用Beautiful Soup来简化这样一个过程。

```python
import requests
from bs4 import BeautifulSoup#从bs4里面导入BeautifulSoup
url="https://www.baidu.com"
content=resquests.get(url).content
soup=BeautifulSoup(content,"lxml") #解析器用lxml,这个我也不是很懂,反正还是要加一下的,这样就创建了一个BeautifulSoup对象,接下来可以对这个对象进行提取想要的数据
print soup #我们可以打印看看
print soup.prettify() #格式化打印,美观,直观一点
'''output:
<html>
	<head>
		<title>
		The Dormouse's story
		</title>
这边只是举个例子'''
```

相对于正则表达式而言,BeautifulSoup对于标签的获取非常的简单,在这里需要掌握的几个方法为

```python
print soup.head
#<head><title>The Dormouse's story</title>
print soup.title
#<title>The Dormouse's story</title>
#相信通过这两个例子也大概知道怎么取标签了
#但是这种方法只能取到第一个符合要求的标签,要取所有的在下面会提到

#但我现在只想取The Dormouse's story 这段怎么办呢?
print soup.title.string
print type(soup.title.string) #Navigable String

#还有一个我觉得蛮好的就是查看属性
print soup.p
#<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
print soup.p.attrs
#{'class':["title"],"name":dromouse}
print soup.p["class"]
#["title"]
#解释一下,我觉得这个和字典非常像,通过key,可以取到对应的value

#再举几个例子,在爬虫中非常常见的需要爬取网页内的链接,那么我们如何得到链接呢,
#一般来说,连接都在<a href="xxx">里面,所以如下:
link=soup.a["href"]
```

当然通常来说,我们需要得到所有的标签,或者所有相应的链接,那么就要用到findAll()方法,

```python
#如要找到所有的a标签
soup.findAll("a")
soup.findAll("a",class_="link") #因为class是python的关键词,所以要加_,这个实际上也就是个过滤的方法,找到所有class="link"的a标签,其他请查看文档,注意findAll返回的是列表
#基本方法就是这样,当然还有许多的参数,可以自行了解
```

## 4.爬虫第二阶段,提交表单

### 1.提交data

这边就直接用requests库写了,我觉得比较友好一点。

大多数网站是动态网页,需要动态的传递参数,最常见的一种就是需要登陆,登陆就需要提交用户名和密码,这也就是提交表单,那么如何实现这样一个过程呢,先看代码。

```python
import requests
url="http://cuiqingcai.com/1319.html"
form_data={"user-name":'xxx',"password":'xxx'}
response=requsts.post(url,data=form_data)
content=response.content
#当然,寻找这个user-name,password,一般还是要去网站上寻找提交的表单是长什么样子的,方法如下：
```

![](http://oam2zfeyb.bkt.clouddn.com/1212.png)

鼠标右键网页－点那个设置，常用首选项里面勾选上启动持续日志，之所以这么做的原因在于，从登陆界面切换到登录后的网页，如果不设置就找不到了，设置完毕之后。

我们以豆瓣为例，到豆瓣的登陆界面，右键审查元素，到网络里面，然后在原网页输入用户名密码，点击登陆，你会得到这样的一个网页：

![](http://oam2zfeyb.bkt.clouddn.com/login.png)

我们要提交的登陆信息,点击login,点击参数就能得到了,form_email和form_password。

以上是有关登陆的问题,当然请求的url,在消息头里面的请求网址:

![](http://oam2zfeyb.bkt.clouddn.com/request.png)

到这步为止代码如下:

```python
import requests
form_data={"form_email":"xxx@163.com","form_password":"xxxx"}
url="https://www.douban.com/accounts/login"
response=requests.post(url=url,data=form_data)
content=response.content
#到这步为止实际上仍然是登陆不上的,我们接着看
```

### 2.提交headers

网站一般是不怎么希望被爬虫爬的,所以就会屏蔽爬虫,这时候我们就要伪装成浏览器,这就到了要需要设置headers的时候了。

一般来说,需要提交的headers为,User-Agent:你的浏览器,Referer:这我也母鸡,Host:主机名,在上面那张图里面的消息头,上传的消息头里面

![](http://oam2zfeyb.bkt.clouddn.com/headers.png)

代码修订如下:

```python
import requests
form_data={"form_email":"xxx@163.com","form_password":"xxxx"}
headers={}
headers["User-Agent"]='Mozilla/5.0 (Windows NT 10.0; WOW64; rv:48.0) Gecko/20100101\ Firefox/48.0'
headers['Referer']="https://www.douban.com/" #这种写法,只是字典的一种写法..
headers["Host"]="www.douban.com"
url="https://www.douban.com/accounts/login"
response=requests.post(url=url,data=form_data,headers=headers)
content=response.content
#到了这步仍然是登陆不上的,因为还有一个验证码的问题,我们可以试着打开一下这个url,就会发现竟然还需要输入验证码...好像有时候也不用输,不用输,那么就搞定了,直接就可以登陆了...
```

### 3.验证码了怎么办

![](http://oam2zfeyb.bkt.clouddn.com/%E9%AA%8C%E8%AF%81%E7%A0%81.png)

鼠标右键审查元素,上图中左上角圈出来的移动到验证码上,此时我们需要两个元素,第一个img id="captcha_img",第二个是'captcha-id',代码如下:

```python
import requests
from bs4 import BeautifulSoup
import re
import urllib
from PIL import Image
form_data={"form_email":"guchenliangsh@163.com","form_password":"gcl19955210"}
headers={}
headers["User-Agent"]='Mozilla/5.0 (Windows NT 10.0; WOW64; rv:48.0) Gecko/20100101\ Firefox/48.0'
headers['Referer']="https://www.douban.com/" 
headers["Host"]="www.douban.com"
url="https://www.douban.com/accounts/login"
response=requests.post(url=url,data=form_data,headers=headers,timeout=30)#timeout也可以在请求里找到
content=response.content
soup=BeautifulSoup(content,"lxml")
#print soup.prettify()
captcha_id=soup.find("input",{"name":"captcha-id"})["value"]
pic=captcha_id=soup.find("img",id="captcha-image")["src"]#这边不能用find_all,因为find_all返回的是一个列表
urllib.urlretrieve(pic,"captcha-img")#保存图片到本地
img=Image.open("captcha-img")#打开图片
img.show()
captcha=raw_input("plz input captcha")#根据图片输入验证码
form_data['captcha-solution'] = captcha  
form_data['captcha-id'] = captcha_id  
response=requests.post(url=url,data=form_data,headers=headers,timeout=30)
content=response.content
```

### 4.Cookie登陆

通常我们在登陆过后,下一次就不需要再输入用户名和密码了,这就是cookie登陆,但是cookie登陆一般有时间限制。

```python
import cookielib
import urllib2
cj=cookielib.CookieJar()
#将一个保存cookie对象和http处理器相绑定
cookie_support=urllib2.HTTPCookieProcessor(cj)
#创建一个opener,设置一个handler用户处理http的url打开
opener=urllib2.bulid_opener(cookie_support,urllib2.HTTPHandler)
#安装opener,以后调用urlopen()会使用安装过的opener对象
urllib2.install_opener(opener)
```

## 3.小结

到这里为止,爬虫的基本知识也就差不多了,一般的网站都能够进行爬取了,动手试试看吧。