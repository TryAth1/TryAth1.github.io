---
layout: post
title: kaggle-HPN Health Prize 冠军队伍文献探讨
date: 2016-7-20
categories: blog
tags: [机器学习]
description: 本篇是对于kaggle2011年9月1日的冠军队伍Market Makers的算法的探讨
---

# 流程

> 1. 从原数据中提取特征
> 2. 预测模型建模
> 3. 模型的融合
> 4. 未来情况的变化      

## 数据处理

### 赛题数据描述

*数据来源于一些病人在某一年进行医疗保险索赔的金额,与这个病人在随后这一年在医院所待的时间。现在给出连续两年的索赔与住院时间数据用来建模,以第三年的索赔金额对病人的住院时间进行预测*

针对赛题**Market Markers**使用了两个不同的模型数据

> 1. **DataSet1:**如果用户在这两年都进行索赔,那么这两年将被作为独立的不同的数据记录.
> 2. **DataSet2**:所有的索赔记录都被记录在一起

## 建模

### 损失函数定义

- ![](C:\Users\guche\Pictures\6.png)

  > 如果我们把使 pred=log(pred+1),act=log(act+1),那么我们就可以得到
  >
  > ![](C:\Users\guche\Pictures\7.png)
  >
  > > 实际上就是预测值与实际值差的一种表示方式
  > >
  > > 由于采取了log的转换,所以在提交结果的时候进行了还原
  > >
  > > > pred_submit=exp(pred)-1

### 采取的模型

1. Gradient Boosting 用于GBDT
2. Neural Networks 神经网络
3. Bagged Tree 对样本数据采取抽样放回的方式构造多棵树,投票选择结果
4. linear Model 线性模型

> 经过测试,在本试题中,表现:*gradient boosting machine > Nuetral Network+bagged tree > linear model*

<u>文章中还提到了capping of the model ,不知道是什么意思</u>

### 最后选取的模型

**最后采取的模型是多个模型的线性融合**

> 原理在于:通过多个模型的融合可以避免过度拟合或者欠拟合的问题.不同模型的错误的方向是不同的,所着重的点也是不一样的,通过模型有效的提高了整体的预测准确率。

- 在模型的融合过程中,通过使用了k-fold cross validation的方法得到多个预测样本

- 如何选择k是一个问题,我们使用了2和12作为k的备选数字,选择2是使得训练的数量减少加快训练的速度,而12需要12个处理器同时进行处理,即涉及到了分布式处理.更多的步骤需要来决定k的值

- 由于我们有两个DataSet,为了融合模型所以我们只能只能选择两个DataSet重合的那部分数据,并选择这部分重合数据的子集进行各种工作

- 为了防止过度拟合,我们随机选择特征的一部分(50%)来进行多次线性回归得到多个基础模型,最后对这些基础模型得到的参数值取平均值得到我们融合模型的权重.然后我们计算了在融合后的模型中基础模型的重要程度,并丢弃掉那么不怎么重要的基础模型.最后决定了一共有20个基础模型是重要的,用这20个基础模型重复这个过程,并得到最后的权重.~~不是很理解~~

  ​

### 未来情况的变化

在预测模型中很重要的一个假设就是未来会和现在相同,但是通常情况不是如此

**例如:**

> 1. 医疗水平的进步人们不需要住院
> 2. 医生的人员流动,他们对于人员的住院情况有一定的影响
> 3. 政策的变化,如生孩子的妇女被鼓励在医院待更多的天数

当然我们希望能够预防这样的情况发生,在我们的特征变量中如果出现比较大的异常值,可以被认为是来自于未来的变量,需要对此进行处理,有以下几种方法:

> 1. 比较训练数据和leaderboard数据的均值
> 2. 找出重复的分布并可视化
> 3. 计算一个单变量的度量来量化两个数据集不同之处,如AUC
> 4. 建造一个分类模型使用所有的变量并计算变量的重要程度

# 附录

## 在建模中选择变量的选择

**目的:**减少不必要的/没有用的变量

**方法:** 使用逻辑回归,使用所有的特征,并计算每一个特征的重要程度,如果某个特征在被移除之后并没有影响模型预测的准确性,那就重复这个过程直到所有包含在模型中的特征都很重要

为了计算模型的重要程度,我们需要去除一个特征,并重新计算模型的准确度,如Gini系数和AUC。这个去除某个特征的过程重复多次,	计算平均的准确率。如果模型的准确度并没有随着随机去除的特征而显著减少,那么这个特征就被认为对模型没有影响可以安全的移除。

去除的总次数应该尽可能的多,至少两次。但这个过程,对于数据量大的情况就会造成大量的计算。

由于是随机去除特征,重复这个过程得到的特征子集是不一样的,但是应该得到相同的模型准确度,如果你再重复这个过程和我们可能得到的是不一样的特征子集,但是也没有关系,因为重要的变量仍然在那里,但是不那么重要的特征就不一样的。 


