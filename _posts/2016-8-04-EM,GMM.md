---
layout: post
title: 从EM算法说起
date: 2016-8-04
categories: blog
tags: [机器学习]
description: 本篇是对EM算法的一个总结,写的挺乱的。
---

# 从EM算法说起

前言:在学习过程中,EM算法确实感觉到非常的复杂,涉及到了许多概率论的知识,在中途看不懂推导过程,故去补了一遍概率论,本篇希望能够涵盖各种基础知识,算是对之前这段时间的一个总结。

## 1.K-means算法

### 简介:

K-means算法实际上理解起来非常简单,K代表一共有K类,means代表均值,就是说将样本集分成按照k个中心点分成k类,具体算法如下:

1. 先随机确定k个中心点

2. 计算每个点到这k个点的距离,并选择最近的那个点进行分类:

   ![](http://oam2zfeyb.bkt.clouddn.com/2-1.png)

3. 分类后重新计算均值

   ![](http://oam2zfeyb.bkt.clouddn.com/2-2.png)

4. 重复2-3步直到收敛

   ​

### 问题:

实际上k-means的算法中,非常关键的一点在于第一步中这k个值的初始选择问题,每一次的选择不同会造成结果的有所不同。

接下来我还想讲一个问题,让我们定义目标函数:

![](http://oam2zfeyb.bkt.clouddn.com/2-3.png)

可以看到这是个非凸函数,所以k-means算法实际上无法求得全局最优值。

#### 补充一、凸函数

设f是定义域为实数的函数,如果对于所有的实数x,f''(x)>=0,那么f是凸函数,当x是向量时,如果其Hessian矩阵H是半正定的(H>=0),那么f是凸函数。

如果f''(x)>0或者H>0,那么称f是严格凸函数。

凸函数在f'(x)=0处可以求得极小值,如:

![](http://oam2zfeyb.bkt.clouddn.com/2-4.png)

## 2.EM算法

给定的训练样本:

![](http://oam2zfeyb.bkt.clouddn.com/2-6.png)

求每个x对应的 隐藏类别z,使得p(x,z)最大。p(x,z)的似然估计如下:

![](http://oam2zfeyb.bkt.clouddn.com/2-7.png)

#### 补充二、似然函数

|         概率         |          似然           |
| :----------------: | :-------------------: |
| 描述了已知参数时,随机变量的输出结果 | 已知随机变量的输出结果,未知参数的可能取值 |

![](http://oam2zfeyb.bkt.clouddn.com/2-5.png)

也可以写成:

![](http://oam2zfeyb.bkt.clouddn.com/2-8.png)

最大似然说白了就是已知了观测结果和分布,求使得出现这些观测结果可能性最大的参数值,通常用对数的形式表示。这边有两个条件,<u>需要知道观测结果和分布</u>。

#### 补充三、边缘概率

已知离散变量的联合分布p(x,y),求p(x):

![](http://oam2zfeyb.bkt.clouddn.com/2-10.png)

**回到p(x,z)的似然估计:**

![](http://oam2zfeyb.bkt.clouddn.com/2-7.png)

由这条公式我们可以得到:

![](http://oam2zfeyb.bkt.clouddn.com/2-11.png)

在补充二、似然函数中提到,如果我们要估计参数,那么我们需要知道观测结果和分布,在这里,z是隐藏分类,分布未知,所以我们对其进行了假设。

#### 补充四、Jensen	不等式

如果f(x)是凸函数,x是随机变量,那么满足不等式:

![](http://oam2zfeyb.bkt.clouddn.com/2-12.png)

#### 补充五、从随机变量x到随机变量y的期望

如果y是离散型随机变量x的映射,即y=g(x),那么x的发生概率与y的发生概率相等。

举个例子:

投一枚硬币,1代表正面,取-1代表负面,出现正面的概率为0.5,即p(x=1)=0.5;

如果投出一枚硬币出现正面拿10元,出现反面得-10元,那么y是x的映射,y=10x,x取,可以很容易的看出,收益出现+10和-10的概率,和硬币出现正反面的概率是一样的。

- **y期望的计算**(x是离散随机变量的时候)

![](http://oam2zfeyb.bkt.clouddn.com/2-13.png)

- **回到EM算法:**

![](http://oam2zfeyb.bkt.clouddn.com/2-14.png)

f(x)=logx是指以e为底的对数函数,f''(x)=-1/x^2<0,所以logx是凹函数,根据jensen不等式

∴f(E(x))>=E(f(x))

![](http://oam2zfeyb.bkt.clouddn.com/2-15.png)

当![](http://oam2zfeyb.bkt.clouddn.com/2-16.png)(c为常数)时,不等式等号成立

![](http://oam2zfeyb.bkt.clouddn.com/2-17.png)

![](http://oam2zfeyb.bkt.clouddn.com/2-18.png)

这也就是说,在我们固定了参数θ后,z的分布Q实际上就是后验概率,这样一来我们有了观测结果有了分布我们就可以计算θ了。

### EM算法流程

循环直到收敛:

E步   1.计算![](http://oam2zfeyb.bkt.clouddn.com/2-18.png)

M步 2.计算 ![](http://oam2zfeyb.bkt.clouddn.com/2-19.png)

## 3.从EM算法到混合高斯模型

### ①高斯判别模型介绍

在异常诊断里运用到了高斯判别模型,如下:

![](http://oam2zfeyb.bkt.clouddn.com/2-20.png)

在高斯分布中,我们知道μ控制中心位置,σ控制形状,在这边引入协方差矩阵的原因在于,找出了特征与特征之间的相关性。(这边就不细说了)

### ②混合高斯模型

与高斯判别模型的区别在于,混合高斯模型是多项式分布,而高斯判别模型是伯努利分布。

![](http://oam2zfeyb.bkt.clouddn.com/2-21.png)

接下来,我们要求出![](http://oam2zfeyb.bkt.clouddn.com/2-22.png)

可以想到,固定其中两个参数,对第三个进行求导,可以求出对应的参数值。

### ③总结

在高斯模型和混合高斯模型我着实纠结了很久,最主要的问题在于训练出模型之后,怎么去分类新的数据。

后来终于理清了头绪如下:

![](http://oam2zfeyb.bkt.clouddn.com/2-23.png)

需要注意的是,在高斯判别模型中,Σ假设是一样的,不同的只是μ0和μ1的值

![](http://oam2zfeyb.bkt.clouddn.com/2-25.png)

在高斯混合模型中,每个高斯模型的Σ都不同。

## 4.EM算法总结

同样都是求参数,如梯度下降法是基于目标函数为损失函数最小化,EM算法是基于使p(x,z)最大,以似然函数的方式求参数。

## 5.再谈K-means

实际上,k-means也可以看作是EM算法的思想,找到隐含分类使得p(x,z)最大,然后利用z将x进行归类。由于事先我们不知道隐含分类,所以如何指派z,如果是直接指派,像目前k-means这样做的,直接给出k个中心点,属于硬指派;或者是像混合高斯分布,多项式分布,即每个z的概率,属于软指派。

一开始给出的z,p(x,z)必然不会最大,那么就需要进行调整,给定z,调整其他的参数使得p(x,z)最大,调整完之后,发现有更好的z进行指定,那么我们重新指定z,循环这个过程,直至收敛,实际上也就是EM算法的思想。



参考:

 [JerryLead的博客 斯坦福大学机器学习读书笔记系列](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)

[manji_lee的博客-GMM,Gaussian Mixture Model](http://blog.csdn.net/manji_lee/article/details/41335307)

[Vamei 博客-数据科学系列](http://www.cnblogs.com/vamei/p/3178534.html)

[zou09的博客-生成模型和判别模型](http://www.cnblogs.com/vamei/p/3178534.html)

后记:EM算法挺饶的,琢磨了3天才稍微能够理解一点,加油吧。



























